{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# how to add full folder imgages in df\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Path to folder containing images\n",
    "folder_path = r\"C:\\Users\\YourUsername\\Documents\\Images\"  # Change this to your actual folder path\n",
    "\n",
    "# Get list of image files\n",
    "image_files = [f for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# Create lists to store filenames and images\n",
    "image_data = []\n",
    "image_labels = []\n",
    "\n",
    "# Read each image, convert to NumPy array, and store\n",
    "for file in image_files:\n",
    "    img_path = os.path.join(folder_path, file)\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read in grayscale (optional)\n",
    "    img = cv2.resize(img, (64, 64))  # Resize all images to 64x64 pixels\n",
    "    img_array = img.flatten()  # Flatten image to a 1D array\n",
    "    \n",
    "    image_data.append(img_array)\n",
    "    image_labels.append(file.split('_')[0])  # Example: Extract label from filename (Modify if needed)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(image_data)\n",
    "df['Label'] = image_labels  # Add labels (Modify if labels are stored separately)\n",
    "\n",
    "# Split into train and test sets\n",
    "X = df.iloc[:, :-1].values  # Features (image pixels)\n",
    "y = df.iloc[:, -1].values   # Labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display shapes of datasets\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct Approach (Train, Validation, and Test Split)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "# Folder locations\n",
    "train_val_dir = r\"C:\\Users\\YourUsername\\Documents\\dataset\\train_val\"  # Contains train+val images\n",
    "test_dir = r\"C:\\Users\\YourUsername\\Documents\\dataset\\test\"  # Separate folder for test images\n",
    "\n",
    "# Load training + validation dataset (80% train, 20% validation)\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    train_val_dir,\n",
    "    validation_split=0.2,  # Split into train (80%) and validation (20%)\n",
    "    subset=\"training\",\n",
    "    seed=42,\n",
    "    image_size=(32, 32),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "val_dataset = image_dataset_from_directory(\n",
    "    train_val_dir,\n",
    "    validation_split=0.2,  # Same split as train dataset\n",
    "    subset=\"validation\",\n",
    "    seed=42,\n",
    "    image_size=(32, 32),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Load test dataset separately (NO validation split here)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    test_dir,  # Only test images\n",
    "    image_size=(32, 32),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Normalize images\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_dataset = val_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "test_dataset = test_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "# 🚀 Summary:\n",
    "# Dataset\tSource Folder\tHow it’s Split\n",
    "# Train\ttrain_val\t80% of train_val folder\n",
    "# Validation\ttrain_val\t20% of train_val folder\n",
    "# Test\ttest\tNo split, separate test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "✅ Method 1: Random Sampling from the Dataset\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"your_dataset.csv\")\n",
    "\n",
    "# Take a subset (e.g., 30% of the original dataset)\n",
    "subset_df = df.sample(frac=0.3, random_state=42)\n",
    "\n",
    "# Assume last column is the target variable\n",
    "X = subset_df.iloc[:, :-1]  # Features\n",
    "y = subset_df.iloc[:, -1]   # Target\n",
    "\n",
    "# Split the subset into training (80%) and testing (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print subset sizes\n",
    "print(f\"Subset Size: {len(subset_df)}, Train Size: {len(X_train)}, Test Size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What Are Frozen Layers in Deep Learning?\n",
    "# 🔹 Frozen layers are layers in a neural network whose weights do not update during training.\n",
    "# 🔹 This is commonly used in transfer learning, where we use a pre-trained model and only train some layers while keeping others fixed (frozen).\n",
    "\n",
    "# Why Freeze Layers?\n",
    "# ✅ To retain pre-learned features from a large dataset (e.g., ImageNet).\n",
    "# ✅ To reduce training time, since fewer parameters are updated.\n",
    "# ✅ To avoid overfitting, especially when the dataset is small.\n",
    "\n",
    "# How to Freeze Layers?\n",
    "# In TensorFlow/Keras, you can freeze layers by setting:\n",
    "\n",
    "layer.trainable = False\n",
    "# This ensures that weights of that layer will not change during training.\n",
    "\n",
    "# Example: Freezing Layers in Transfer Learning\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "# Load the pre-trained model (without the top classification layers)\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze all layers in the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False  # Freezing layers\n",
    "\n",
    "# Add new layers on top\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(10, activation=\"softmax\")  # Output layer for classification\n",
    "])\n",
    "\n",
    "# Check trainable parameters\n",
    "print(\"Trainable Parameters:\", sum([np.prod(var.shape) for var in model.trainable_weights]))  # Should be only for new layers\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# # When to Freeze and Unfreeze Layers?\n",
    "# 1️⃣ When dataset is small → Freeze most layers, train only the last few.\n",
    "# 2️⃣ When dataset is large & similar → Unfreeze more layers for fine-tuning.\n",
    "# 3️⃣ Fine-tuning → First train with frozen layers, then unfreeze some layers and retrain with a low learning rate.\n",
    "\n",
    "# 📌 Summary\n",
    "# Action\tEffect\n",
    "# Freeze Layers (trainable=False)\tWeights remain unchanged during training.\n",
    "# Unfreeze Layers (trainable=True)\tWeights are updated during training.\n",
    "# Used in Transfer Learning?\t✅ Yes, for feature reuse & faster training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Here’s a complete step-by-step guide to adding files and folders to Jupyter Notebook (in Anaconda). I’ll cover:\n",
    "\n",
    "Adding files and folders in Jupyter\n",
    "\n",
    "Uploading ZIP files and extracting\n",
    "\n",
    "Handling different file types (CSV, images, text, etc.)\n",
    "\n",
    "Creating a folder structure\n",
    "\n",
    "Splitting datasets (e.g., into training/testing)\n",
    "\n",
    "🔧 1. Launching Jupyter Notebook from Anaconda\n",
    "Step 1:\n",
    "\n",
    "Open Anaconda Navigator.\n",
    "\n",
    "Launch Jupyter Notebook.\n",
    "\n",
    "A browser window will open at http://localhost:8888/tree.\n",
    "\n",
    "📁 2. Creating Folders in Jupyter Notebook\n",
    "Steps:\n",
    "\n",
    "Click on the \"New\" dropdown (top-right).\n",
    "\n",
    "Select \"Folder\" → A new folder appears named Untitled Folder.\n",
    "\n",
    "Rename it by selecting the checkbox → Rename.\n",
    "\n",
    "Example folder structure to follow:\n",
    "\n",
    "kotlin\n",
    "Copy\n",
    "Edit\n",
    "project/\n",
    "│\n",
    "├── data/\n",
    "│   ├── raw/\n",
    "│   ├── processed/\n",
    "│\n",
    "├── models/\n",
    "├── notebooks/\n",
    "├── utils/\n",
    "📄 3. Uploading Files (CSV, TXT, Images, etc.)\n",
    "Steps:\n",
    "\n",
    "Click Upload (top-right).\n",
    "\n",
    "Browse and select files (e.g., data.csv, image.jpg, text.txt).\n",
    "\n",
    "Click the Upload button next to the filename.\n",
    "\n",
    "📦 4. Uploading and Extracting ZIP Files\n",
    "Steps:\n",
    "\n",
    "Upload the .zip file via the Upload button.\n",
    "\n",
    "In a notebook, run:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import zipfile\n",
    "\n",
    "# Path to zip file and extraction\n",
    "zip_path = \"your_file.zip\"\n",
    "extract_path = \"data/\"\n",
    "\n",
    "# Extract\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "✅ Extracts everything to the data/ folder.\n",
    "\n",
    "📂 5. Reading Different File Types in Jupyter\n",
    "a) CSV File\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/raw/data.csv')\n",
    "print(df.head())\n",
    "b) Excel File\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df = pd.read_excel('data/raw/data.xlsx')\n",
    "c) Image File (for CV Projects)\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = Image.open('data/raw/image.jpg')\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "d) Text File\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "with open('data/raw/sample.txt', 'r') as f:\n",
    "    content = f.read()\n",
    "    print(content)\n",
    "✂️ 6. Splitting Data into Train/Test Folders (Image or Text)\n",
    "For Image Classification Projects:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set paths\n",
    "source_folder = \"data/raw_images/\"\n",
    "train_folder = \"data/train/\"\n",
    "test_folder = \"data/test/\"\n",
    "\n",
    "# Create folders if not exist\n",
    "for folder in [train_folder, test_folder]:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# Get list of images\n",
    "images = os.listdir(source_folder)\n",
    "train_imgs, test_imgs = train_test_split(images, test_size=0.2, random_state=42)\n",
    "\n",
    "# Move files\n",
    "for img in train_imgs:\n",
    "    shutil.copy(os.path.join(source_folder, img), train_folder)\n",
    "\n",
    "for img in test_imgs:\n",
    "    shutil.copy(os.path.join(source_folder, img), test_folder)\n",
    "For CSV Data (ML Projects):\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/raw/data.csv')\n",
    "\n",
    "# Split\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save\n",
    "train.to_csv('data/processed/train.csv', index=False)\n",
    "test.to_csv('data/processed/test.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
